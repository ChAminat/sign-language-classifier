## Sign-language-detector

Классификация жестов американского языка жестов (ASL) на основе датасета Sign
Language MNIST

## Постановка задачи

Задача состоит в распределении изображений жестов рук по 24 классам,
соответствующим буквам американского языка жестов (A-Z, исключая J и Z, которые
требуют движения). Классификация происходит на основе изображения жеста размером
28x28 пикселей в градациях серого. Автоматическое распознавание жестов может
помочь людям с нарушениями слуха лучше коммуницировать с помощью компьютерных
приложений.

Проект также полезен для образовательных целей: на задаче классификации
изображений можно разобраться в том, как устроены сверточные нейронные сети
(CNN), отточить принципы работы с изображениями, а также увидеть, как выбор
архитектуры модели и ее гиперпараметров влияет на качество предсказания.

## Формат входных и выходных данных

Входные данные:

Датасет Sign Language MNIST, используемый при обучении, имеет следующую
структуру: всего имеется 27455 объектов для тренировки и 7172 для теста. Данные
представлены в формате CSV с заголовком: `label`, `pixel1`, `pixel2`, ...,
`pixel784`. Каждая строка соответствует одному изображению 28x28 пикселей (784
пикселя) в градациях серого со значениями от 0 до 255.

Выходные данные:

Модель предсказывает букву (класс) жеста как одно из 24 возможных значений
(0-25, исключая 9 и 25).

## Метрики

В задаче используются две основные метрики:

1. **Accuracy** — доля правильных ответов. Поскольку датасет сбалансированный,
   accuracy хорошо отражает качество модели.
2. **Weighted F1-score** — взвешенная F1-мера, которая учитывает точность и
   полноту по каждому классу. Полезна для более детальной оценки качества
   модели.

## Валидация

Поскольку в исходном датасете есть только train и test части, была проведена
предобработка данных и деление сета train в пропорции 80% на 20% для выделения
валидационных данных. Во время валидации мы смотрим, как падает loss и растут
accuracy и F1-score. Для визуализации используется система логирования
[MLflow](https://mlflow.org/), а также сохраняются локальные графики в
директорию `plots/`.

## Данные

### Данные

В задаче используется датасет жестов [Sign Language MNIST]
(https://www.kaggle.com/datasets/datamunge/sign-language-mnist).

Датасет сбалансированный по классам (примерно по 1000 изображений на класс в
train и по 250 в test). Получен путем аугментации оригинальных изображений
жестов (различные фильтры, яркость, контраст, повороты), что делает его более
разнообразным и приближенным к реальным условиям.

## Моделирование

### Бейзлайн

1. Модель должна справляться лучше чем модель, выдающая случайный класс в
   качестве предсказания (accuracy > 1/24 ≈ 4.2%);
2. Для сравнения в качестве бейзлайна можно использовать логистическую регрессию
   на сырых пикселях или простую полносвязную сеть.

### Основная модель

За основу данного проекта был взят
[ноутбук с Kaggle](https://www.kaggle.com/code/vijaypro/cnn-pytorch-96),
демонстрирующий точность 96% на тестовых данных.

0. **Предобработка данных**

Данные из .csv файлов загружаются с помощью pandas. Каждая строка содержит метку
класса и 784 значения пикселей. Мы преобразуем их в тензоры и изменяем форму до
(28, 28). Затем изображения увеличиваются до 224x224 с помощью OpenCV и
нормализуются делением на 255.0.

1. **Модель:**

Архитектура сверточной нейронной сети (CNN) состоит из 5 сверточных блоков и
полносвязных слоев:

```python
self.model = nn.Sequential(
    # Conv1
    nn.Conv2d(in_channels, 32, 5),
    nn.MaxPool2d(2),
    nn.ReLU(),
    nn.BatchNorm2d(32),

    # Conv2
    nn.Conv2d(32, 64, 5),
    nn.MaxPool2d(2),
    nn.ReLU(),
    nn.BatchNorm2d(64),

    # Conv3
    nn.Conv2d(64, 128, 3),
    nn.MaxPool2d(2),
    nn.ReLU(),
    nn.BatchNorm2d(128),

    # Conv4
    nn.Conv2d(128, 256, 3),
    nn.MaxPool2d(2),
    nn.ReLU(),
    nn.BatchNorm2d(256),

    # Dropout
    nn.Dropout(0.1),

    # Conv5
    nn.Conv2d(256, 512, 3),
    nn.MaxPool2d(2),
    nn.ReLU(),
    nn.BatchNorm2d(512),

    # Classifier
    nn.Flatten(),
    nn.Linear(512 * 4 * 4, 256),
    nn.Dropout(0.1),
    nn.Linear(256, num_classes)
)
```

2. В качестве оптимизатора используется Adam;

3. Минимизируем функцию потерь CrossEntropyLoss;

4. Модель обучается 5 эпох (параметр настраивается в конфиге train.yaml).

## Внедрение

Модель может использоваться как пакет для классификации изображений жестов. В
проекте осуществляется перевод обученной модели в формат .onnx для дальнейшей
работы над моделью как продуктом.

# Работа с проектом

## Setup

Клонирование репозитория:

```
git clone https://github.com/ChAminat/sign-language-detector.git
```

После клонирования репозитория перейдите в его папку:

```
cd sign-language-classifier
```

Создайте новое окружение для работы и перейдите в него
([гайд по установке conda](https://www.anaconda.com/docs/getting-started/anaconda/install)):

```
conda create -n signenv  python=3.12
conda activate signenv
```

Установите uv для управления зависимостями:

```
pip install uv
```

Установка зависимостей проекта (для установки пакетов разработки добавить флаг
--dev):

```
uv sync
```

Установка хуков для pre-commit:

```
uv run pre-commit install
```

Запустите проверки всех файлов:

```
uv run pre-commit run -a
```

## Train

Для отслеживания метрик в ходе обучения и валидации используется MLflow.
Запустите MLflow сервер в отдельном терминале:

```
mlflow server --host 127.0.0.1 --port 8080
```

Скачайте данные для обучения и тестирования модели. Для этого из корня запустите
скрипт скачивания командой:

```
# через скрипт Python:
uv run python sign_language_classifier/data_load/download_data.py
```

Запустите модель обучаться:

```
uv run python sign_language_classifier/train/train.py
```

Для изменения параметров обучения используйте Hydra:

```
python train.py training.lr=0.0005 training.num_epochs=30
```

## Infer

После того, как модель обучится, запустите систему предсказаний на тестовых
данных с помощью модели, сохраненной во время обучения в папке checkpoints в
файле .ckpt (сейчас там лежит модель после пяти эпох тренировки
epoch=05-val_accuracy=1.0000-v1).

Название вашего файла может отличаться. Пример запуска (скрипт, путь до данных,
путь до модели):

```
uv run python sign_language_classifier/infer/infer.py data/sign_mnist_test.csv checkpoints/epoch=05-val_accuracy=1.0000-v1.ckpt
```

Обученную модель можно перевести из .ckpt в формат .onnx (скрипт, путь до
чекпоинта, путь сохранения преобразованной модели):

```
uv run python sign_language_classifier/modules/onnx_compilation.py checkpoints/epoch=05-val_accuracy=1.0000-v1.ckpt
```
